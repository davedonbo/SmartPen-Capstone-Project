{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rxXV5Qi0qkA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, lfilter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D,Conv2D,MaxPooling2D, MaxPooling1D, LSTM, Bidirectional, Flatten, Dense, Dropout, Input, GlobalAveragePooling1D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Resampling function to get all time series to the same number of steps for compatibility with neural networks\n",
    "def resample(data, target_length=64):\n",
    "    return np.interp(\n",
    "        np.linspace(0, len(data), target_length),\n",
    "        np.arange(len(data)),\n",
    "        data)\n",
    "\n",
    "\n",
    "def standardize_per_sample(data):\n",
    "    means = data.mean(axis=1, keepdims=True)\n",
    "    stds = data.std(axis=1, keepdims=True)\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    eps = 1e-8\n",
    "    data_out = (data - means) / (stds + eps)\n",
    "\n",
    "    return data_out\n",
    "\n",
    "\n",
    "def normalise_data_per_sample(train, test):\n",
    "    train_out = standardize_per_sample(train)\n",
    "    test_out = standardize_per_sample(test)\n",
    "    return train_out, test_out\n",
    "\n",
    "\n",
    "# Preprocessing function for all samples(filtering)\n",
    "def preprocess_data_filter_high(data):\n",
    "    processed = []\n",
    "    for sample in data:\n",
    "        filtered_sample = np.array([butter_highpass_filter(feature) for feature in sample.T]).T\n",
    "        processed.append(filtered_sample)\n",
    "    processed = np.array(processed, dtype=np.float32)\n",
    "    return processed\n",
    "\n",
    "\n",
    "def preprocess_filter_moving_average(data, window_size=5):\n",
    "    def moving_average_filter(signal, window_size):\n",
    "        return np.convolve(signal, np.ones(window_size) / window_size, mode='same')\n",
    "\n",
    "    processed = []\n",
    "    for sample in data:\n",
    "        filtered_sample = np.array([moving_average_filter(feature, window_size) for feature in sample.T]).T\n",
    "        processed.append(filtered_sample)\n",
    "    return np.array(processed, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Preprocessing function for all samples(resampling)\n",
    "def preprocess_data_resample(data):\n",
    "    processed = []\n",
    "    for sample in data:\n",
    "        resampled_sample = np.array([resample(feature) for feature in sample.T]).T\n",
    "        processed.append(resampled_sample)\n",
    "    processed = np.array(processed, dtype=np.float32) \n",
    "    return processed\n",
    "\n",
    "# Function to plot confusion matrix. This will be used at the very end to visualise the errors in some predictions of the alphabets\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, class_names):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "    plt.xlabel(\"Predicted Alphabets\")\n",
    "    plt.ylabel(\"True Alphabets\")\n",
    "    plt.show()\n",
    "\n",
    "# Load already splitted dataset\n",
    "X = np.load('X_train_Shuffled.npy', allow_pickle=True)\n",
    "y = np.load('y_train_Shuffled.npy', allow_pickle=True)\n",
    "\n",
    "nac_X_train = np.load('nac_X_train.npy', allow_pickle=True)\n",
    "nac_y_train = np.load('nac_y_train.npy', allow_pickle=True)\n",
    "nac_X_test = np.load('nac_X_test.npy', allow_pickle=True)\n",
    "nac_y_test = np.load('nac_y_test.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKu0I7EX1qZM",
    "outputId": "fc10d2dc-8d63-4234-a1f4-f7d35ca8ac6b"
   },
   "outputs": [],
   "source": [
    "#Checking the average length of the time series data in order to inform me on the number of neurons to use for each layer\n",
    "#for the various NN models I have. Also to inform on resample size\n",
    "\n",
    "sequence_lengths = [seq.shape[0] for seq in X_train]\n",
    "\n",
    "# Calculate average number of time steps\n",
    "average_time_steps = np.median(sequence_lengths)\n",
    "\n",
    "print(\"average time steps: \",average_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "aOpp5h9u2Sv0",
    "outputId": "8a4d2201-ec2a-4cc8-883a-e15dcb537c40"
   },
   "outputs": [],
   "source": [
    "# Choosing one sample to plot\n",
    "sample_index = 0  \n",
    "sample_data = X_train[sample_index]\n",
    "\n",
    "#Below are the features in their respective order(acc=accelerometer, gyro=gyroscope each in 3 axis)\n",
    "features = ['Accx','Accy','Accz','Gyrox','Gyroy','Gyroz']\n",
    "X_train_heat = preprocess_data_resample(X_train)\n",
    "print(X_train_heat.shape)\n",
    "\n",
    "# Averaging features over time steps to get in order to visualise their correlations in the correlation matrix\n",
    "data_averaged = X_train_heat.mean(axis=1)\n",
    "\n",
    "# Computing the correlation matrix\n",
    "correlation_matrix = np.corrcoef(data_averaged.T)\n",
    "\n",
    "# Plotting the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", xticklabels=[f\"{features[i]}\" for i in range(6)], yticklabels=[f\"{features[i]}\" for i in range(6)])\n",
    "plt.title(\"Correlation Matrix of Features (Averaged Across Time Steps)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two X_train and y_train datasets for nac\n",
    "nac_X_train = preprocess_data_resample(nac_X_train)\n",
    "nac_X_test = preprocess_data_resample(nac_X_test)\n",
    "\n",
    "nac_X_train = preprocess_filter_moving_average(nac_X_train)\n",
    "nac_X_test = preprocess_filter_moving_average(nac_X_test)\n",
    "\n",
    "nac_X_train, nac_X_test = normalise_data_per_sample(nac_X_train,nac_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gjEOFQOx2VMO",
    "outputId": "703a9b98-e00c-463e-f06f-52251667fd6c"
   },
   "outputs": [],
   "source": [
    "#picking a random sample for visualisation\n",
    "sample_index = 15\n",
    "sample_data_raw = X_train[sample_index]\n",
    "\n",
    "# Plotting each feature as a separate line\n",
    "plt.figure(figsize=(12, 6))\n",
    "for feature_idx in range(sample_data_raw.shape[1]):\n",
    "    plt.plot(sample_data_raw[:, feature_idx], label=f\"{features[feature_idx]}\")\n",
    "plt.title(f\"Time-Series Data for Sample {sample_index} (Raw)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Sensor Value\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# resampling time steps to 52 based on ealier calculation\n",
    "X_train = preprocess_data_resample(X_train) #resample\n",
    "X_test = preprocess_data_resample(X_test)\n",
    "sample_data_resample = X_train[sample_index]\n",
    "\n",
    "# Showing nature of sensor values after resampling\n",
    "plt.figure(figsize=(12, 6))\n",
    "for feature_idx in range(sample_data_resample.shape[1]):\n",
    "    plt.plot(sample_data_resample[:, feature_idx], label=f\"{features[feature_idx]}\")\n",
    "plt.title(f\"Time-Series Data for Sample {sample_index} (Resampled to 45 time steps)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Sensor Value\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# filtering data moving average\n",
    "X_train = preprocess_filter_moving_average(X_train)\n",
    "X_test = preprocess_filter_moving_average(X_test)\n",
    "\n",
    "\n",
    "sample_data_filter = X_train[sample_index]\n",
    "\n",
    "# Showing nature of data after filtering\n",
    "plt.figure(figsize=(12, 6))\n",
    "for feature_idx in range(sample_data_filter.shape[1]):\n",
    "    plt.plot(sample_data_filter[:, feature_idx], label=f\"{features[feature_idx]}\")\n",
    "plt.title(f\"Time-Series Data for Sample {sample_index} (Filtered with moving average filter with window of 5)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Sensor Value\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# standardising features per sample\n",
    "X_train, X_test = normalise_data_per_sample(X_train, X_test)\n",
    "sample_data_normalised = X_train[sample_index]\n",
    "\n",
    "# Showing nature of data after normalising\n",
    "plt.figure(figsize=(12, 6))\n",
    "for feature_idx in range(sample_data_normalised.shape[1]):\n",
    "    plt.plot(sample_data_normalised[:, feature_idx], label=f\"{features[feature_idx]}\")\n",
    "plt.title(f\"Time-Series Data for Sample {sample_index} (normalised)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Sensor Value\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # variance plot\n",
    "import pandas as pd\n",
    "def plot_imu_variance(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    sample_idx,\n",
    "    source='train',\n",
    "    window=5,\n",
    "    threshold=0.5\n",
    "):\n",
    "    if source.lower() == 'train':\n",
    "        sample = x_train[sample_idx]\n",
    "        title_str = f\"Mean Rolling Variance (window={window}) - Train Sample #{sample_idx}\"\n",
    "    else:\n",
    "        sample = x_test[sample_idx]\n",
    "        title_str = f\"Mean Rolling Variance (window={window}) - Test Sample #{sample_idx}\"\n",
    "\n",
    "    columns = ['AccX', 'AccY', 'AccZ','GyroX','GyroY','GyroZ']\n",
    "    df = pd.DataFrame(sample, columns=columns)\n",
    "\n",
    "    rolling_var = df.rolling(window=window, center=True).var()\n",
    "\n",
    "    variances = rolling_var.mean(axis=1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(variances, label='Mean Rolling Variance')\n",
    "    plt.title(title_str)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Mean Variance Across Acc & Gyro Axes\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.axhline(threshold, color='red', linestyle='--', label=f'Threshold = {threshold}')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_imu_variance(X_train, X_test, sample_idx=sample_index, source='train', window=10, threshold=0.15)\n",
    "\n",
    "\n",
    "\n",
    "#segment to retain active region\n",
    "from scipy.signal import resample\n",
    "\n",
    "def segment_writing_phases(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    window_size=5,\n",
    "    threshold=0.5\n",
    "):\n",
    "\n",
    "    def find_active_segment(sample, window_size, threshold):\n",
    "        # Convert the sample to a DataFrame for rolling variance\n",
    "        df = pd.DataFrame(\n",
    "            sample,\n",
    "            columns=[\"Accx\", \"Accy\", \"Accz\",'GyroX','GyroY','GyroZ']\n",
    "        )\n",
    "\n",
    "        # Rolling variance across time, then mean over 6 axes\n",
    "        rolling_var = df.rolling(window=window_size, center=True).var().mean(axis=1)\n",
    "\n",
    "        # Boolean mask where variance > threshold\n",
    "        active_mask = rolling_var > threshold\n",
    "        active_indices = np.where(active_mask)[0]\n",
    "\n",
    "        if len(active_indices) == 0:\n",
    "            # No active region\n",
    "            return None, None\n",
    "        else:\n",
    "            return active_indices[0], active_indices[-1]\n",
    "\n",
    "    def truncate_samples(x_array):\n",
    "        truncated_list = []\n",
    "        for sample in x_array:\n",
    "            start_idx, end_idx = find_active_segment(sample, window_size, threshold)\n",
    "            if start_idx is not None:\n",
    "                # Truncate to [start_idx : end_idx + 1]\n",
    "                truncated_sample = sample[start_idx:end_idx+1]\n",
    "            else:\n",
    "                # No active region found => return the whole sample\n",
    "                truncated_sample = sample\n",
    "            truncated_list.append(truncated_sample)\n",
    "        return truncated_list\n",
    "\n",
    "    # 1) Truncate train and test\n",
    "    truncated_train_list = truncate_samples(x_train)\n",
    "    truncated_test_list = truncate_samples(x_test)\n",
    "\n",
    "    # 2) Compute median length *from the training set only*\n",
    "    train_lengths = [len(s) for s in truncated_train_list]\n",
    "    median_len_train = int(np.median(train_lengths)) if len(train_lengths) > 0 else 0\n",
    "\n",
    "    def resample_to_length(sample_list, desired_length):\n",
    "        resampled_list = []\n",
    "        for samp in sample_list:\n",
    "            if desired_length > 0:\n",
    "                # Resample along time dimension (axis=0)\n",
    "                samp_resampled = resample(samp, desired_length, axis=0)\n",
    "            else:\n",
    "                # If desired_length = 0, produce an empty shape (0, 6)\n",
    "                samp_resampled = np.zeros((0, samp.shape[1]), dtype=samp.dtype)\n",
    "            resampled_list.append(samp_resampled)\n",
    "        return resampled_list\n",
    "    print(\"median len: \",median_len_train)\n",
    "\n",
    "    # 3) Resample both train and test to the same median length from train\n",
    "    resampled_train_list = resample_to_length(truncated_train_list, 46) #supposed to be median_len_train\n",
    "    resampled_test_list  = resample_to_length(truncated_test_list,  46)\n",
    "\n",
    "    # 4) Stack the list of resampled arrays into a single np.ndarray\n",
    "    #    Now all samples have the same time dimension\n",
    "    x_train_out = np.stack(resampled_train_list, axis=0)\n",
    "    x_test_out  = np.stack(resampled_test_list,  axis=0)\n",
    "\n",
    "    return x_train_out, x_test_out\n",
    "\n",
    "X_train_truncated, X_test_truncated = segment_writing_phases(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    window_size=10,\n",
    "    threshold=0.15\n",
    ")\n",
    "\n",
    "\n",
    "# picking up sample to show\n",
    "sample_data_truncated = X_train_truncated[sample_index]  # Shape: (time_steps, features)\n",
    "\n",
    "# Showing nature of data after normalising\n",
    "plt.figure(figsize=(12, 6))\n",
    "for feature_idx in range(sample_data_truncated.shape[1]):\n",
    "    plt.plot(sample_data_truncated[:, feature_idx], label=f\"{features[feature_idx]}\")\n",
    "plt.title(f\"Time-Series Data for Sample {sample_index} (truncated)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Sensor Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging main dataset wtih nac\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Merge the X_train datasets along the first axis (samples)\n",
    "X_train = np.concatenate((X_train, nac_X_train), axis=0)\n",
    "X_test = np.concatenate((X_test, nac_X_test), axis=0)\n",
    "\n",
    "# Merge the y_train datasets\n",
    "y_train = np.concatenate((y_train, nac_y_train), axis=0)\n",
    "y_test = np.concatenate((y_test, nac_y_test), axis=0)\n",
    "\n",
    "# Shuffle the merged datasets while maintaining correspondence between X and y\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NsKHM-Ka3j99",
    "outputId": "7e20de02-5f12-4d6a-bf9e-cb55491fdc2f"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Checking dimensions of the input after preprocessing\n",
    "print(f\"Processed X_train shape: {len(X_train)} samples, first sample shape: {X_train[0].shape}\")\n",
    "print(f\"Processed X_test shape: {len(X_test)} samples, first sample shape: {X_test[0].shape}\")\n",
    "\n",
    "# Encoding labels using LabelEncoder so there is no hierarchy of weights for each label\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Ensuring input shape is appropriate\n",
    "time_steps = X_train.shape[1]\n",
    "features = X_train.shape[2]\n",
    "num_classes = 27 #since there are 26 alphabets and 1 nac(not a character)\n",
    "\n",
    "\n",
    "early_stopping_cnn = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "early_stopping_lstm = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "early_stopping_bilstm = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "\n",
    "\n",
    "#for each of the models below, our decision on number of layers, neurons, regularisation, etc were based on research and our own\n",
    "#experimentation and tweaking parameters for optimal results. \n",
    "\n",
    "# CNN Model\n",
    "def build_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(128, kernel_size=32, activation='relu'),\n",
    "        MaxPooling1D(pool_size=16), \n",
    "        Dropout(0.6),\n",
    "        Flatten(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# LSTM Model\n",
    "def build_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(128,unroll=True, return_sequences=False),\n",
    "        Dropout(0.6),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# BiLSTM Model\n",
    "def build_bilstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Bidirectional(LSTM(128)),\n",
    "        Dropout(0.6),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Compiling all models with the Adam optimiser\n",
    "cnn_model = build_cnn_model((time_steps, features), num_classes)\n",
    "cnn_model.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "lstm_model = build_lstm_model((time_steps, features), num_classes)\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bilstm_model = build_bilstm_model((time_steps, features), num_classes)\n",
    "bilstm_model.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Training and evaluating the CNN model\n",
    "print(\"Training CNN model...\")\n",
    "cnn_history = cnn_model.fit(X_train, y_train_encoded, validation_data=(X_test, y_test_encoded), epochs=5000, batch_size=32, callbacks=[early_stopping_cnn])\n",
    "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Training and evaluating the LSTM model\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_history = lstm_model.fit(X_train, y_train_encoded, validation_data=(X_test, y_test_encoded), epochs=5000, batch_size=32, callbacks=[early_stopping_lstm])\n",
    "lstm_test_loss, lstm_test_accuracy = lstm_model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"LSTM Test Accuracy: {lstm_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train and evaluating the BiLSTM model\n",
    "print(\"Training BiLSTM model...\")\n",
    "bilstm_history = bilstm_model.fit(X_train, y_train_encoded, validation_data=(X_test, y_test_encoded), epochs=5000, batch_size=32, callbacks=[early_stopping_bilstm])\n",
    "bilstm_test_loss, bilstm_test_accuracy = bilstm_model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"BiLSTM Test Accuracy: {bilstm_test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HJJHOYSt1AxG",
    "outputId": "cc7f442c-3665-4473-c3ea-767ab0bc0783"
   },
   "outputs": [],
   "source": [
    "#confusion matrices for each model\n",
    "class_names = label_encoder.classes_\n",
    "# Predicting and generating confusion matrix for CNN\n",
    "cnn_y_pred = np.argmax(cnn_model.predict(X_test), axis=1)\n",
    "plot_confusion_matrix(y_test_encoded, cnn_y_pred, \"CNN\", class_names)\n",
    "\n",
    "# Predicting and generating confusion matrix for LSTM\n",
    "lstm_y_pred = np.argmax(lstm_model.predict(X_test), axis=1)\n",
    "plot_confusion_matrix(y_test_encoded, lstm_y_pred, \"LSTM\", class_names)\n",
    "\n",
    "# Predicting and generating confusion matrix for BiLSTM\n",
    "bilstm_y_pred = np.argmax(bilstm_model.predict(X_test), axis=1)\n",
    "plot_confusion_matrix(y_test_encoded, bilstm_y_pred, \"BiLSTM\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, class_names):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normalize to percentages (0-100)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Increase figure size and adjust annotations\n",
    "    plt.figure(figsize=(14, 12)) \n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm_percent, \n",
    "        annot=True, \n",
    "        fmt='.1f',    \n",
    "        annot_kws={'size': 12}, \n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names,\n",
    "        cbar=False,\n",
    "        square=True        \n",
    "    )\n",
    "    \n",
    "    # Adjust label and title font sizes\n",
    "    plt.title(f'Confusion Matrix for {model_name} (%)', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    \n",
    "    # Rotate tick labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(rotation=0, fontsize=12)\n",
    "    \n",
    "    # Add padding to prevent cutoff\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# CNN\n",
    "cnn_y_pred = np.argmax(cnn_model.predict(X_test), axis=1)\n",
    "plot_confusion_matrix(y_test_encoded, cnn_y_pred, \"CNN\", class_names)\n",
    "\n",
    "# LSTM\n",
    "lstm_y_pred = np.argmax(lstm_model.predict(X_test), axis=1)\n",
    "plot_confusion_matrix(y_test_encoded, lstm_y_pred, \"LSTM\", class_names)\n",
    "\n",
    "# BiLSTM\n",
    "bilstm_y_pred = np.argmax(bilstm_model.predict(X_test), axis=1)\n",
    "plot_confusion_matrix(y_test_encoded, bilstm_y_pred, \"BiLSTM\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(f\"LSTM Test Accuracy: {lstm_test_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(f\"BiLSTM Test Accuracy: {bilstm_test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1NVyZpxjgJQM",
    "outputId": "e7447112-6411-4bff-8b91-0ffed6e5fc16"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name):\n",
    "    # Extract data from history\n",
    "    acc = history.history.get('accuracy')\n",
    "    val_acc = history.history.get('val_accuracy')\n",
    "    loss = history.history.get('loss')\n",
    "    val_loss = history.history.get('val_loss')\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Plotting Accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(model_name+' Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plotting Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "    plt.title( model_name+' Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(cnn_history, \"CNN\")\n",
    "\n",
    "plot_training_history(lstm_history, \"LSTM\")\n",
    "\n",
    "plot_training_history(bilstm_history, \"BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "#Print macro-averaged, micro-averaged, or weighted metrics\n",
    "macro_precision = precision_score(y_test_encoded, cnn_y_pred, average='macro')\n",
    "macro_recall    = recall_score(y_test_encoded, cnn_y_pred, average='macro')\n",
    "macro_f1        = f1_score(y_test_encoded, cnn_y_pred, average='macro')\n",
    "\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:   \", macro_recall)\n",
    "print(\"Macro F1-score: \", macro_f1)\n",
    "\n",
    "# Print a classification report (precision, recall, F1 for each class + overall)\n",
    "print(\"\\nDetailed classification report :\")\n",
    "print(classification_report(y_test_encoded, cnn_y_pred, target_names=class_names))\n",
    "\n",
    "################################################################################################\n",
    "# Print macro-averaged, micro-averaged, or weighted metrics\n",
    "macro_precision = precision_score(y_test_encoded, lstm_y_pred, average='macro')\n",
    "macro_recall    = recall_score(y_test_encoded, lstm_y_pred, average='macro')\n",
    "macro_f1        = f1_score(y_test_encoded, lstm_y_pred, average='macro')\n",
    "\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:   \", macro_recall)\n",
    "print(\"Macro F1-score: \", macro_f1)\n",
    "\n",
    "# Print a classification report (precision, recall, F1 for each class + overall)\n",
    "print(\"\\nDetailed classification report LSTM:\")\n",
    "print(classification_report(y_test_encoded, lstm_y_pred, target_names=class_names))\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "# Print macro-averaged, micro-averaged, or weighted metrics\n",
    "macro_precision = precision_score(y_test_encoded, bilstm_y_pred, average='macro')\n",
    "macro_recall    = recall_score(y_test_encoded, bilstm_y_pred, average='macro')\n",
    "macro_f1        = f1_score(y_test_encoded, bilstm_y_pred, average='macro')\n",
    "\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:   \", macro_recall)\n",
    "print(\"Macro F1-score: \", macro_f1)\n",
    "\n",
    "# Print a classification report (precision, recall, F1 for each class + overall)\n",
    "print(\"\\nDetailed classification report BiLSTM:\")\n",
    "print(classification_report(y_test_encoded, bilstm_y_pred, target_names=class_names))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
